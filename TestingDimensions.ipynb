{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get code from WorkBook.ipynb\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "def conv(ninputs, nfilters, kernel_size):\n",
    "    return nn.Conv1d(ninputs, nfilters, kernel_size, padding=1, bias=True)\n",
    "\n",
    "def maxpool(filter_size, padding):\n",
    "    return nn.MaxPool1d(filter_size, padding=padding)\n",
    "\n",
    "def activation():\n",
    "    return nn.ReLU()\n",
    "\n",
    "def dataloading(path, filename, filetype=\"CSV\"):\n",
    "    \"\"\" Returns a pandas dataframe from a data file\n",
    "        Args:\n",
    "            - path: a Path() object pointing to the directory containing the data\n",
    "            - filename: a file name in CSV format located in the directory pointed \n",
    "            to by the Path() object\n",
    "            - filetype: specify the type of file to read from (at the moment, the\n",
    "            only possibility is \"CSV\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: we should raise exception here if filetype is not CSV\n",
    "    \n",
    "    file = path / filename\n",
    "    if filetype == \"CSV\":\n",
    "        return pd.read_csv(file)\n",
    "    else:\n",
    "        print('Only CSV file are supported at the moment.')\n",
    "        return False\n",
    "\n",
    "def createxandy(data, window, p_window, firstcolastimestamp=True):\n",
    "    \"\"\" Returns X and y as torch tensors to be used in our training and predictions\n",
    "        Args:\n",
    "            - data: loaded data in a pandas dataframe format\n",
    "            - window: the length of each X item\n",
    "            - p_window: the length of the prediction window (i.e. how many timesteps in the\n",
    "            future we want to predict)\n",
    "            - firstcolastimestamp: whether our firstcolumn consists of timestamp (default to True)\n",
    "    \"\"\"\n",
    "    \n",
    "    if firstcolastimestamp:\n",
    "        data = data.iloc[:,1:].values\n",
    "    else:\n",
    "        data = data.values\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    # X will be a list of sequences of size window\n",
    "    # y will be a list of sequences of size p_window, \n",
    "    # immediately following the corresponding X\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    # Total sequence to go over is N + 1 - window - p_window\n",
    "    seq = N + 1 - window - p_window\n",
    "    for i in range(seq):\n",
    "        X_temp, y_temp = data[i:i+window], data[i+window:i+window+p_window]\n",
    "        X.append(X_temp)\n",
    "        y.append(y_temp)\n",
    "        \n",
    "    X, y = torch.Tensor(X).float(), torch.Tensor(y).float()\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def createdatasets(X, y, split=[0.7,0.3,0.0]):\n",
    "    \"\"\" Returns a list of Dataset objects (for training, validation and testing)\n",
    "        Args:\n",
    "            - X: tensor containing independent variables\n",
    "            - y: tensor containing dependent variables\n",
    "            - split: list containing the split between training, validation \n",
    "            and testing datasets (the total should add to 1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: we should raise exception here\n",
    "    assert sum(split) - 1.0 < 1e-10\n",
    "\n",
    "    train_size, valid_size, _ = split\n",
    "    \n",
    "    N = len(X)\n",
    "    \n",
    "    trainidx = int(len(X) * train_size)\n",
    "    valididx = int(len(X) * (train_size + valid_size))\n",
    "    \n",
    "    # TODO: what if we don't want test / valid datasets? \n",
    "    \n",
    "    train_ds = TensorDataset(X[:trainidx], y[:trainidx])\n",
    "    if valid_size == 0.0:\n",
    "        valid_ds = False\n",
    "    else:\n",
    "        valid_ds = TensorDataset(X[trainidx:valididx], y[trainidx:valididx])\n",
    "    if valid_size + train_size == 1.0:\n",
    "        test_ds = False\n",
    "    else:\n",
    "        test_ds = TensorDataset(X[valididx:], y[valididx:])\n",
    "    \n",
    "    return [train_ds, valid_ds, test_ds]\n",
    "\n",
    "def createdataloaders(datasets, bs=64):\n",
    "    \"\"\" Create the dataloaders for all the datasets and returns a list of \n",
    "        dataloaders and/or False when no dataloaders can be created for a given \n",
    "        dataset (for example when dataset is empty)\n",
    "        Args:\n",
    "            - datasets: a list of Dataset objects in this order: training, \n",
    "            validation, testing\n",
    "            - bs: batch size (64 by default)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: ensure datasets is of the correct object type\n",
    "    # TOCHECK: should we have shuffle=True or False for validation / testing?\n",
    "    \n",
    "    train_dl = DataLoader(datasets[0], batch_size=bs, shuffle=False)\n",
    "    if datasets[1]:\n",
    "        valid_dl = DataLoader(datasets[1], batch_size=bs, shuffle=True)\n",
    "    else:\n",
    "        valid_dl = False\n",
    "    if datasets[2]:\n",
    "        test_dl = DataLoader(datasets[2], batch_size=bs, shuffle=True)\n",
    "    else:\n",
    "        test_dl = False\n",
    "    \n",
    "    return [train_dl, valid_dl, test_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 32\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "\n",
    "path = Path('Data')\n",
    "filename = 'data2.csv'\n",
    "window = 10\n",
    "p_window = 3\n",
    "\n",
    "data = dataloading(path, filename)\n",
    "X, y = createxandy(data, window, p_window, firstcolastimestamp=True)\n",
    "datasets = createdatasets(X, y, split=[0.7,0.3,0.0])\n",
    "dataloaders = createdataloaders(datasets, bs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first batch\n",
    "Xb, yb = next(iter(dataloaders[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the layers one by one\n",
    "# 2 is the number of variables in the time series\n",
    "\n",
    "conv1 = conv(2, n_filters, kernel_size)\n",
    "pool1 = maxpool(kernel_size, padding)\n",
    "conv2 = conv(n_filters, n_filters, kernel_size)\n",
    "pool2 = maxpool(kernel_size, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 10, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 32, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's run the first batch through the different layers\n",
    "# We need to permute the dimensions since we consider the number of variables in the\n",
    "# time series to be the number for channels\n",
    "# TODO: try something different by only considering one channel. In this case, we need to \"add\"\n",
    "# a dimension to the time series (through torch.unsqueeze())\n",
    "resconv1 = conv1(Xb.permute(0,2,1))\n",
    "resconv1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 32, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respool1 = pool1(resconv1)\n",
    "respool1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 32, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resconv2 = conv2(respool1)\n",
    "resconv2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 32, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respool2 = pool2(resconv2)\n",
    "resconv2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = resconv2.view(resconv2.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 is the number of variables in the time series here\n",
    "res = resconv2.view(resconv2.shape[0],2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a linear layer now\n",
    "# 64 is basically n_filters * 2 (i.e. number of variables in the time series)\n",
    "\n",
    "linear = nn.Linear(64, p_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reslin = linear(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reslin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does it work? At least we have the correct dimensions\n",
    "yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert reslin.permute(0,2,1).shape == yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
