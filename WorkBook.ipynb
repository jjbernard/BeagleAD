{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WorkBook\n",
    "\n",
    "In this workbook we develop the key components before moving them to Python code. The goal is to interactively develop the code before moving it to pure Python while refactoring it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We need to first load the data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key variables for data loading\n",
    "dirpath = Path('Data')\n",
    "filename = 'data.csv'\n",
    "path = dirpath / filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22695, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-02 21:15:00</td>\n",
       "      <td>73.967322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-02 21:20:00</td>\n",
       "      <td>74.935882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-02 21:25:00</td>\n",
       "      <td>76.124162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-02 21:30:00</td>\n",
       "      <td>78.140707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-02 21:35:00</td>\n",
       "      <td>79.329836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp      value\n",
       "0  2013-12-02 21:15:00  73.967322\n",
       "1  2013-12-02 21:20:00  74.935882\n",
       "2  2013-12-02 21:25:00  76.124162\n",
       "3  2013-12-02 21:30:00  78.140707\n",
       "4  2013-12-02 21:35:00  79.329836"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time series in the data is the number of columns minus 1\n",
    "# The first column contains the timestamp\n",
    "nb_ts = data.shape[1] - 1\n",
    "\n",
    "# TOCHECK: what if the data has no timestamp?\n",
    "# TOCHECK: it looks like we don't need this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is the total length of the dataset. We could use len() here\n",
    "N = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data has to be manipulated as a numpy array to fit into a torch tensor later\n",
    "data = data.iloc[:,1:].values\n",
    "\n",
    "# TOCHECK: what should we do with the timestamp (when there is one)? Here we drop it, but probably wrong..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[73.96732207],\n",
       "       [74.935882  ],\n",
       "       [76.12416182],\n",
       "       ...,\n",
       "       [97.13546835],\n",
       "       [98.05685212],\n",
       "       [96.90386085]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define X and y, given that f(X) = y. \n",
    "# The goal of the neural network is the model f()\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call w the size of the window we will use on the time series to predict the output (i.e. the size of X)\n",
    "# Let's call p_w the number of steps in the future we want to predict\n",
    "w = 10\n",
    "p_w = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With w and p_w set, we can now define the number of items that will populate X and y\n",
    "nitems = N + 1 - w - p_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's not populate X and y\n",
    "for i in range(nitems):\n",
    "    X_temp, y_temp = data[i:i+w], data[i+w:i+w+p_w]\n",
    "    X.append(X_temp)\n",
    "    y.append(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nitems == len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the dataset into training and validation set\n",
    "# We cannot do that randomly, the validation dataset needs to follow the training dataset\n",
    "train_size = 0.8\n",
    "\n",
    "idxvalid = int(nitems * train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's move the data to a torch tensor\n",
    "X, y = torch.Tensor(X).float(), torch.Tensor(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[0] == nitems\n",
    "assert X.shape[1] == w\n",
    "assert X.shape[2] == nb_ts\n",
    "assert y.shape[0] == nitems\n",
    "assert y.shape[1] == p_w\n",
    "assert y.shape[2] == nb_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(X[:idxvalid], y[:idxvalid])\n",
    "valid_ds = TensorDataset(X[idxvalid:], y[idxvalid:])\n",
    "\n",
    "# TOCHECK: should we move to an IterableDataset() instead? See https://pytorch.org/docs/stable/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the DataLoaders\n",
    "\n",
    "bs = 64\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=False)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_dl))\n",
    "\n",
    "# TOCHECK: why is a a list and not a tensor??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some functions\n",
    "\n",
    "def conv(ninputs, nfilters, kernel_size):\n",
    "    return nn.Conv1d(ninputs, nfilters, kernel_size, padding=1, bias=True)\n",
    "\n",
    "def maxpool(filter_size, padding):\n",
    "    return nn.MaxPool1d(filter_size, padding=padding)\n",
    "\n",
    "def activation():\n",
    "    return nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading routine that returns data from a CSV file (initially)\n",
    "\n",
    "def dataloading(path, filename, filetype=\"CSV\"):\n",
    "    \"\"\" Returns a pandas dataframe from a data file\n",
    "        Args:\n",
    "            - path: a Path() object pointing to the directory containing the data\n",
    "            - filename: a file name in CSV format located in the directory pointed \n",
    "            to by the Path() object\n",
    "            - filetype: specify the type of file to read from (at the moment, the\n",
    "            only possibility is \"CSV\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: we should raise exception here if filetype is not CSV\n",
    "    \n",
    "    file = path / filename\n",
    "    if filetype == \"CSV\":\n",
    "        return pd.read_csv(file)\n",
    "    else:\n",
    "        print('Only CSV file are supported at the moment.')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates our dependent and independent variables\n",
    "\n",
    "def createxandy(data, window, p_window, firstcolastimestamp=True):\n",
    "    \"\"\" Returns X and y as torch tensors to be used in our training and predictions\n",
    "        Args:\n",
    "            - data: loaded data in a pandas dataframe format\n",
    "            - window: the length of each X item\n",
    "            - p_window: the length of the prediction window (i.e. how many timesteps in the\n",
    "            future we want to predict)\n",
    "            - firstcolastimestamp: whether our firstcolumn consists of timestamp (default to True)\n",
    "    \"\"\"\n",
    "    \n",
    "    if firstcolastimestamp:\n",
    "        data = data.iloc[:,1:].values\n",
    "    else:\n",
    "        data = data.values\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    # X will be a list of sequences of size window\n",
    "    # y will be a list of sequences of size p_window, \n",
    "    # immediately following the corresponding X\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    # Total sequence to go over is N + 1 - window - p_window\n",
    "    seq = N + 1 - window - p_window\n",
    "    for i in range(seq):\n",
    "        X_temp, y_temp = data[i:i+window], data[i+window:i+window+p_window]\n",
    "        X.append(X_temp)\n",
    "        y.append(y_temp)\n",
    "        \n",
    "    X, y = torch.Tensor(X).float(), torch.Tensor(y).float()\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's create the datasets we need\n",
    "\n",
    "def createdatasets(X, y, split=[0.7,0.3,0.0]):\n",
    "    \"\"\" Returns a list of Dataset objects (for training, validation and testing)\n",
    "        Args:\n",
    "            - X: tensor containing independent variables\n",
    "            - y: tensor containing dependent variables\n",
    "            - split: list containing the split between training, validation \n",
    "            and testing datasets (the total should add to 1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: we should raise exception here\n",
    "    assert sum(split) - 1.0 < 1e-10\n",
    "\n",
    "    train_size, valid_size, _ = split\n",
    "    \n",
    "    N = len(X)\n",
    "    \n",
    "    trainidx = int(len(X) * train_size)\n",
    "    valididx = int(len(X) * (train_size + valid_size))\n",
    "    \n",
    "    # TODO: what if we don't want test / valid datasets? \n",
    "    \n",
    "    train_ds = TensorDataset(X[:trainidx], y[:trainidx])\n",
    "    if valid_size == 0.0:\n",
    "        valid_ds = False\n",
    "    else:\n",
    "        valid_ds = TensorDataset(X[trainidx:valididx], y[trainidx:valididx])\n",
    "    if valid_size + train_size == 1.0:\n",
    "        test_ds = False\n",
    "    else:\n",
    "        test_ds = TensorDataset(X[valididx:], y[valididx:])\n",
    "    \n",
    "    return [train_ds, valid_ds, test_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the dataloaders\n",
    "\n",
    "def createdataloaders(datasets, bs=64):\n",
    "    \"\"\" Create the dataloaders for all the datasets and returns a list of \n",
    "        dataloaders and/or False when no dataloaders can be created for a given \n",
    "        dataset (for example when dataset is empty)\n",
    "        Args:\n",
    "            - datasets: a list of Dataset objects in this order: training, \n",
    "            validation, testing\n",
    "            - bs: batch size (64 by default)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: ensure datasets is of the correct object type\n",
    "    # TOCHECK: should we have shuffle=True or False for validation / testing?\n",
    "    \n",
    "    train_dl = DataLoader(datasets[0], batch_size=bs, shuffle=False)\n",
    "    if datasets[1]:\n",
    "        valid_dl = DataLoader(datasets[1], batch_size=bs, shuffle=True)\n",
    "    else:\n",
    "        valid_dl = False\n",
    "    if datasets[2]:\n",
    "        test_dl = DataLoader(datasets[2], batch_size=bs, shuffle=True)\n",
    "    else:\n",
    "        test_dl = False\n",
    "    \n",
    "    return [train_dl, valid_dl, test_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a model\n",
    "n_filters = 32\n",
    "kernel_size = 3\n",
    "padding = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('Data')\n",
    "filename = 'data2.csv'\n",
    "window = 10\n",
    "p_window = 3\n",
    "\n",
    "data = dataloading(path, filename)\n",
    "X, y = createxandy(data, window, p_window, firstcolastimestamp=True)\n",
    "datasets = createdatasets(X, y, split=[0.7,0.3,0.0])\n",
    "dataloaders = createdataloaders(datasets, bs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume the number of variable in the time series is the number of channels\n",
    "# So for univariate time series, the number of channels is going to be 1\n",
    "# Let's define a function that will create a model based on the parameters from the time series\n",
    "\n",
    "# Stolen from fast.ai\n",
    "\n",
    "# How do I pass nb_var into the Lambda class / resize function?\n",
    "\n",
    "nb_var = 2\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x): return self.func(x)\n",
    "\n",
    "def flatten(x): return x.view(x.shape[0], -1)\n",
    "\n",
    "def resize(x): return x.view(x.shape[0], nb_var, -1)\n",
    "\n",
    "# We probably should not use flatten here as it creates a tensor with the wrong dimensions\n",
    "\n",
    "def get_model(nb_var, window, p_window, n_filters = 32, kernel_size = 3, lr=0.5):\n",
    "    \"\"\" Create and return the DeepAnt model as a pytorch model\n",
    "        Args:\n",
    "            - nb_var: number of variables in the time series (i.e. 1 for univariate)\n",
    "            - n_filters: number of 1D convolution filters (default to 32)\n",
    "            - kernel_size: size of the convolution filters (default to 3)\n",
    "            - lr = learning rate for the optimizer\n",
    "    \"\"\"\n",
    "    padding = 1\n",
    "    model = nn.Sequential(conv(nb_var, n_filters, kernel_size), activation(),\n",
    "                        maxpool(kernel_size, padding),\n",
    "                        conv(n_filters, n_filters, kernel_size), activation(),\n",
    "                        maxpool(kernel_size, padding), Lambda(resize),\n",
    "                        nn.Linear(nb_var * n_filters, p_window), activation())\n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "    return model, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model(2, window, p_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "  (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "  (6): Lambda()\n",
       "  (7): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (8): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get some data through the model\n",
    "train_dl = dataloaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb, yb = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 10, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following will fail: test = model(Xb), we need to rearrange the dimensions with tensor.permute()\n",
    "a = Xb.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the model directly on the input won't work. We need to create a class and call forward\n",
    "# on the input data. \n",
    "\n",
    "class DeepAntModule(nn.Module):\n",
    "    def __init__(self, conv_size):\n",
    "        super().__init__()\n",
    "        self.padding = 1\n",
    "        self.layers = nn.ModuleList([conv(conv_size[0], conv_size[1], conv_size[2]), \n",
    "                                    activation(), maxpool(conv_size[2], self.padding)])\n",
    "        \n",
    "    def forward(self, Xb):\n",
    "        for layer in self.layers:\n",
    "            Xb = layer(Xb)\n",
    "            \n",
    "        return Xb\n",
    "\n",
    "class FullyConnectedModule(nn.Module):\n",
    "    def __init__(self, nb_ts, n_filters, p_window):\n",
    "        super().__init__()\n",
    "        self.nb_ts = nb_ts\n",
    "        self.n_filters = n_filters\n",
    "        self.p_window = p_window\n",
    "        self.fc = nn.Linear(self.n_filters, self.p_window)\n",
    "        \n",
    "    def forward(self, Xb):\n",
    "        Xb = Xb.view(Xb.shape[0], self.nb_ts, -1)\n",
    "        Xb = self.fc(Xb)\n",
    "        \n",
    "        return Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(nb_var, p_window, n_filters = 32, kernel_size = 3, lr=0.5):\n",
    "    \"\"\" Create and return the DeepAnt model as a pytorch model\n",
    "        Args:\n",
    "            - nb_var: number of variables in the time series (i.e. 1 for univariate)\n",
    "            - p_window: the number of time steps in the future we want to predict\n",
    "            - n_filters: number of 1D convolution filters (default to 32)\n",
    "            - kernel_size: size of the convolution filters (default to 3)\n",
    "            - lr = learning rate for the optimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    model = nn.Sequential(DeepAntModule([nb_var, n_filters, kernel_size]), \n",
    "                          DeepAntModule([n_filters, n_filters, kernel_size]), \n",
    "                          FullyConnectedModule(nb_var, n_filters, p_window))\n",
    "    \n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    return model, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_var is 2 (bivariate time series)\n",
    "# p_window is 3, we try to predict 3 time steps in the future\n",
    "\n",
    "model, opt = get_model(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2, 10])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "if test.permute(0,2,1).shape == yb.shape:\n",
    "    print('OK')\n",
    "else:\n",
    "    print('KO')\n",
    "    print('Output shape is: ', test.shape)\n",
    "    print('Expected shape is: ', yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
